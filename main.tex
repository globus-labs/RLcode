\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}

\usepackage[letterpaper,margin=1.00in]{geometry}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{amsmath,amssymb}
\usepackage{times}
\usepackage{booktabs}
\usepackage{framed,url}
\usepackage{paralist}
\usepackage{listings}
\usepackage[colorlinks,linkcolor=blue, citecolor=blue]{hyperref}
\usepackage{doi} 	% For making DOI's clickable in bibliography
\usepackage[numbers,sort&compress]{natbib}
\usepackage{graphicx}
%\usepackage{multirow}
%\usepackage{pdfsync}
%\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames]{xcolor}
%\usepackage{bbm}
%\usepackage{verbatim} % Allows for comment environment
\usepackage{wrapfig}
%\usepackage{pdfpages}           % For inserting pdf pages and colors
%\usepackage{mdwlist}
%\usepackage{refcheck}           % For checking references
%\usepackage{enumitem}
%\usepackage{outline}
%\usepackage{xspace}
%\usepackage{paralist}
\usepackage{pgfgantt}
\usepackage{pifont}
\usepackage{lipsum}
\usepackage{wasysym}
%\usepackage{MnSymbol}
\usepackage[warn]{textcomp}
\usepackage[font=small,labelfont=bf,textfont=bf]{caption}
\usepackage{paralist}
\usepackage{siunitx}
\sisetup{detect-all}
\usepackage{appendix}
\usepackage{comment}

\usetikzlibrary{shapes.misc, positioning}

% For discussion questions?
\newcommand\q[1]{{\color{blue}[Q: #1]}}
\newcommand\yadu[1]{{\color{red}[Yadu: #1]}}




\title{Reinforcement Learning Workflow and Requirements for Massively Parallel Implementation}
\author{}
\date{November 2019}

\begin{document}

\maketitle

\tableofcontents

\section{Introduction}

The goal here is to describe briefly the reinforcement learning (RL) pipeline that we are developing for electrolyte design and other purposes,
and, from there, develop requirements for HPC system software.


\section{The reinforcement learning pipeline}\label{sec:overview} 


The RL pipelines comprises the four high-level components shown in Figure~\ref{fig:pipe}. These components operate in a pipeline, as follows.

\begin{itemize}
\itemsep-0.2em 
\item
The \emph{generator} constructs large numbers of candidate designs by using one or more RL agents to explore incrementally the target design space.
It uses a cheap method (e.g., a simple ML model) to obtain rough estimates of design quality and passes on promising candidates to the next phase.

\item
The \emph{scorer} applies one or more methods to each design passed to it by the generator, 
and the passes the design plus the resulting values (the design's ``score'') to the selector. 
The methods used here for scoring are typically ML models that can be executed quickly, because they must be applied to many designs.
% These are typically lower-cost (and thus lower-accuracy) methods because they need to be applied to many designs.

\item
The \emph{selector} chooses designs to be passed to the simulator.
A variety of strategies may be applied here. 
For example, following an active learning strategy, it may select a mix of high-scoring and high-uncertainty designs.

\item
The \emph{evaluator} applies more expensive methods to the designs passed on by the selector, 
with the goal of obtaining accurate estimates of various properties. 
In the work that we perform here, these methods always involve simulations, but in other cases they could involve experiments.

\end{itemize}

\begin{figure}[h]
  \centering
  %{\setlength{\fboxsep}{0pt}\fbox{%
  \includegraphics[width=0.9\textwidth,trim=0in 0in 0in 0in,clip]{./Figs/pipe.png}
  %}}
  \vspace{-1.5ex}
  \caption{A high-level view of the RL pipeline.}
\label{fig:pipe}
\end{figure}


The results computed by the evaluator must eventually feed back to the other phases to influencing the future behavior of the generator, scorer, and/or evaluator. 
(For example, simulation outputs may be used to retrain the ML models used in the generator and scoring phases.)
The RL process is then repeated.
We discuss the feedback process in later sections.

The complete pipeline thus involves a variety of different computations, including RL generation,  ML inference, ML training, and simulation.
We will typically want many of these computations to to run at once, to make good use of a large computer and avoid excessive file system accesses.


\section{Concepts and terminology}

Our goal is to identify interesting \textbf{objects} in an extremely large space of possible objects,
where ``interesting" is defined in terms of \textbf{properties}, $P_i$.
For example, if the objects are molecules, we might define interesting in terms of three properties: energy, stability, and toxicity.
Thus, for any candidate molecule $m$, we want to know $P_{\textrm{energy}}$($m)$, $P_{\textrm{stability}}$($m)$, and $P_{\textrm{toxicity}}$($m)$.

We typically cannot determine $P_i(m)$ precisely. Instead, for a property $P_i$, we have various \textbf{methods}, $P_i^k$, 
each of which can be used to obtain estimates of $P_i(m)$ values. In the molecular design case, a method could be, for example, 
a particular experimental protocol,
a quantum mechanics (QM) code,
a database collected from the literature, or
a machine-learned (ML) model based on estimated values produced by any other method.
During the discovery process, a method $P_i^k$ may be applied to a series of objects. 
We denote the \textbf{values} computed by a method $P_i^k$ as \textbf{V}($P_i^k$).

\subsection{Relationships among methods}

We may use multiple methods for the same property, each with different characteristics.
Important characteristics can include the set of objects to which it can be applied (its domain); 
its computational costs for different objects; and its accuracies and uncertainties for different objects.
For example, for a property $E$, we might have:

\begin{itemize}\itemsep-0.2em 
\item
$E^{\textrm{G4}}$, a QM code that produces high-fidelity DFT-G4MP2 calculations of $E$. 
This model produces high-accuracy estimates of $E$, but is expensive to run, and thus its domain of applicability is limited to smaller molecules.
%Expensive to run (hours), higher accuracy; domain limited by cost
\item
$E^{\textrm{B3}}$, a QM code that produces lower-fidelity DFT-B3LYP calculations of $E$. This model produces lower-accuracy estimates of $E$ than
does $E^{\textrm{G4}}$, but is cheaper to run. Nevertheless, its computational costs still limit its domain of applicability.
\item
$E^{\textrm{ml(G4)}}$, an ML model trained on \textbf{V}($E^{\textrm{G4}}$).
This model is cheap to run but has lower accuracy than either of the first two methods. It is expensive to train.
\item
$E^{\textrm{ml(B3)}}$, an ML model trained on \textbf{V}($E^{\textrm{B3}}$).
This model is is less accurate than $E^{\textrm{ml(G4)}}$, but otherwise has similar characteristics.
\item
$E^{\textrm{$\Delta$(B3, G4)}}$, a delta-learning ML model trained on \textbf{V}($E^{\textrm{B3}}$) and  \textbf{V}($E^{\textrm{G4}}$) 
that can be applied to a value $E^{\textrm{B3}}(m)$ to obtain an estimate of $E^{\textrm{G4}}(m)$.
This model is also cheap to run and expensive to train; its accuracy is between that of $E^{\textrm{B3}}$ and $E^{\textrm{G4}}$ and its domain is \textbf{V}($E^{\textrm{B3}}$).
\end{itemize}

If one method, $Q$, is defined relative to another, $P$, we say $P\prec Q$.
Thus, in our example,
$E^{\textrm{B3}} \prec E^{\textrm{ml(B3)}}$, $E^{\textrm{G4}} \prec E^{\textrm{ml(G4)}}$, and
$E^{\textrm{B3}}, E^{\textrm{G4}} \prec E^{\textrm{$\Delta$(B3, G4)}}$: see Figure~\ref{fig:deps}.

\subsection{Updating methods}

\begin{wrapfigure}[8]{r}[0pt]{0.2\textwidth}
\vspace{-3ex}
  \centering
  %{\setlength{\fboxsep}{0pt}\fbox{%
  \includegraphics[width=0.2\textwidth,trim=0in 0in 0in 0in,clip]{./Figs/deps.png}
  %}}
  \vspace{-4ex}
  \caption{Method dependencies.
\label{fig:deps}}
\end{wrapfigure}

A new value for a method $Q$, i.e., an addition to \textbf{V}($Q$), may motivate updates to ML methods $Q'$ s.t. $Q \prec Q'$. 
(We say ``may'' because we likely will not want to update methods for every new value.)
For example:
\begin{itemize}\itemsep-0.2em 
\item
A new $E^{\textrm{G4}}$ value may motivate retraining (i.e., updating) $E^{\textrm{ml(G4)}}$ and/or $E^{\textrm{$\Delta$(B3, G4)}}$.
\item
In the case of delta-learning methods, a new value for one method may alter the domain of another.
For example, a new $E^{\textrm{B3}}$  value, $v$, increases the domain of $E^{\textrm{$\Delta$(B3, G4)}}$ if $v \notin \textbf{V}(E^{\textrm{G4}})$
\end{itemize}

When a method $Q$ is updated, we may want to discard \textbf{V}($Q$);
how frequently to do this is a question of cost/accuracy tradeoffs.

\subsection{Method servers, value servers, and trainers}

\begin{wrapfigure}[11]{r}[0pt]{0.22\textwidth}
\vspace{-2ex}
  \centering
  %{\setlength{\fboxsep}{0pt}\fbox{%
  \includegraphics[width=0.22\textwidth,trim=0in 0in 0in 0in,clip]{./Figs/servers.png}
  %}}
  \vspace{-4ex}
  \caption{Method server and other components.
\label{fig:server}}
\end{wrapfigure}

The Generator, Scorer, and Evaluator components in Figure~\ref{fig:pipe} call various methods to estimate different properties of objects.
We find it useful to introduce three additional types of components to encapsulate the major operations associated with methods,
namely executing a method on an object, storing the resulting values, and (in the case of ML methods) retraining.  
Figure~\ref{fig:server} shows these three components. 
We describe each in turn and then examine their relationships.
%We provide more detailed descriptions in Section~\ref{sec:details}.



\begin{itemize}
\itemsep-0.3em 
\item
A \textbf{method server} responds to requests to apply a method to a design and return a value. 
A method server is represented by a pentagon, $\pentagon$, labeled with the name of the method: in this case, \emph{M}. 
\item
A \textbf{value server} responds to requests to record and retrieve values computed by a particular method.
A value server is represented by a circle, \textbigcircle,
labeled \textbf{V}(\emph{M}), where \emph{M} is the name of the method for which the server records values.
\item
A \textbf{trainer} responds to requests to retrain its associated model with supplied data.
A trainer is represented as a rounded rectangle, \begin{tikzpicture}\draw[rounded corners] (0, 0) rectangle (0.6, 0.3) {}; \end{tikzpicture}, labeled \textbf{T}(\emph{M}),
where \emph{M} is once again the name of the method for which it performs training.
\end{itemize}

\q{Do we want method servers to update value servers directly, or have an overall manager handle that?}



As shown in Figure~\ref{fig:server}, these three components are connected. 
A method server \emph{M} is typically associated with a value server \textbf{V}(\emph{M}) used to record values computed by \emph{M}.
In the case of machine-learned methods, an associated trainer \textbf{T}(\emph{M}) may be used to update the method when new values become available,
in which case the method server \emph{M} needs to respond also to requests to update its method.
We may also want to be able to request the associated value server to archive data computed with the previous method.



\subsection{More on the generator}

\begin{wrapfigure}[11]{r}[0pt]{0.22\textwidth}
\vspace{-2ex}
  \centering
  %{\setlength{\fboxsep}{0pt}\fbox{%
  \includegraphics[width=0.22\textwidth,trim=0in 0in 0in 0in,clip]{./Figs/rl.png}
  %}}
  \vspace{-4ex}
  \caption{Reinforcement learning process.
\label{fig:rl}}
\end{wrapfigure}

The inputs to the generator are a reward function, $R$ (e.g., $E^{\textrm{ml(B3)}}$); an initial Q-function; an initial state, $s$ (e.g., a promising molecule); and a Q-function
retraining frequency. 
The generator then repeatedly deepens the design space, as follows (see Figure~\ref{fig:rl}).

\begin{itemize}\itemsep-0.2em 
\item
For each action $a$ that is possible from current state $s$:
\begin{compactitem}
\item
Determine new state $s'$
\item
Use $R$ to determine reward for $s'$
\item
Use $Q$ to estimate value for $s'$: val($s'$) = max$_a$ $Q$($s'$, $a$)
\item
Record ($s$, $s'$, $R$($s'$)) in \textbf{V}($Q$)
\item
Select new state $s'$, e.g., the one with largest $R$($s'$) + val($s'$)
\end{compactitem}
\item
Periodically, update $Q$ by retraining based on available \textbf{V}($Q$). Retraining can be synchronous or asynchronous.
\item
Stop when termination criteria (e.g., max number of steps) reached
\end{itemize}

\noindent
The outputs are a set of states, \textbf{V}($Q$), and a retrained $Q$.


%The RL system comprises four high-level components, as follows:
%
%\begin{itemize}
%\itemsep-0.2em 
%\item
%The \emph{generator} constructs new designs by using one or more RL agents to explore incrementally the target design space.
%
%\item
%The \emph{scorer} determines scores for each of the designs produced by the generator, by applying various methods.
%These are typically lower-cost (and thus lower-accuracy) methods because they need to be applied to many designs.
%
%\item
%The \emph{selector} chooses scored designs to be passed to the simulator.
%A variety of strategies may be applied here. For example, following an active learning strategy, it may select a mix of high-scoring and high-uncertainty designs.
%
%\item
%The \emph{evaluator} applies one or several more expensive methods to the designs passed on by the selector, with the goal of obtaining more accurate 
%values for the properties that those methods determine. 
%In the work that we perform here, these methods always involve simulations, but in other cases they could involve experiments.
%
%\end{itemize}
%
%The results computed by the evaluator must eventually feedback to the other phases to influencing the future behavior of the generator, scorer, and/or evaluator. 
%We explain how this feedback occurs in later sections.



\section{Implementing the RL architecture: An electrolyte design example}

The computational requirements of a specific instantiation of the high-level architecture of Figure~\ref{fig:pipe} will vary greatly
according to the nature of the problem being solved.
We use a specific example to illustrate issues. 

\subsection{Logical architecture}

\begin{figure}
  \centering
  %{\setlength{\fboxsep}{0pt}\fbox{%
  \includegraphics[width=0.9\textwidth,trim=0in 0in 0in 0in,clip]{./Figs/sketch.png}
  %}}
  \vspace{-1.5ex}
  \caption{A logical view of a RL pipeline used for electrolyte design. Models A, B, and C are ML models and M and N are simulation models.}
\label{fig:sketch}
\end{figure}

Figure~\ref{fig:sketch} shows the logical architecture of the materials discovery workflow for an electrolyte discovery application.
This includes the four components of Figure~\ref{fig:pipe} plus six methods: 
the Q-agent, three ML methods (\textbf{A}, \textbf{B}, \textbf{C}), and two simulation methods (\textbf{M}, \textbf{N}).
%The various components can run synchronously or asynchronously, depending on whether an RL agent that generates a set of designs, \emph{X}, does or does not wait, respectively, for A and C to be updated based on \textbf{M}(\emph{X’}) and \textbf{N}(\emph{X’}) (\emph{X’} $\subset$ \emph{X})  before starting the next cycle. We need to evaluate how this effects results obtained.

Initial estimates of runtimes for these components are as follows. 
The generator runs repeatedly, using RL methods to generate perhaps \num{20000} candidates in 30 minutes.
Each candidate is represented by a SMILES string at this point, so space requirements are small.
It may take perhaps 1 minute to score those candidates, assuming a few milliseconds per scoring operation,
and a few seconds to select the top \num{1000} candidates for simulation.
The simulation component is then the most expensive, requiring perhaps 10 node-hours to simulate each candidate,
for a total of \num{100000} node hours.
The other expensive task is retraining models \textbf{B} and \textbf{C}, each of which can take 24 hours on 100 Theta nodes.
While only 2400 node hours each, this is a substantial elapsed time.


\begin{figure}
  \centering
  %{\setlength{\fboxsep}{0pt}\fbox{%
  \includegraphics[width=0.8\textwidth,trim=0in 0in 0in 0in,clip]{./Figs/arch.png}
  %}}
  \vspace{-1.5ex}
  \caption{A schematic of a proposed implementation architecture}
\label{fig:arch}
\end{figure}

\subsection{Implementation architecture}

We now turn to the question of how to implement the logical architecture, for example on a large parallel computer. 
We propose the architecture shown in Figure~\ref{fig:arch}, which we now outline, from top to bottom.

%\textbf{Generator, Scorer, Selector, Evaluator}: 
The upper part of the figure shows the four components that we introduced in Section~\ref{sec:overview}:
the generator, scorer, selector, and evaluator. 
As described earlier, designs are passed from the generator to the Scorer, Selector, and Evaluator.
We envision each of these components operating as a single process.
%with state (objects) either passed between the elements via message passing or via the Store, 

As described earlier, these components evaluate designs by applying various methods. 
We envision this process involving calls to method servers, each of which is responsible for coordinating the dispatch of computational requests
to an Executor and of values returned by those requests to the associated value server.
These may each be a single process, with state (e.g., values) maintained in Store.

The Executor accepts requests to run tasks, tracks their status, and reports results. 
(As tasks may range in size from a few ms on one core to 1000s of core-hours, we may want several executors with different capabilities.) 
The Store maintains state. (We may want different levels of persistence.)
The Executor(s) and Store(s) manage access to perhaps tens of thousands of nodes and many terabytes of storage.

\subsection{An example configuration}

The structure shown in Figure~\ref{fig:sketch} might result in the following configuration. 
We have a total of 19 persistent services, as follows:
\begin{itemize}\itemsep-0.2em
\item
The generator, scorer, selector, and evaluator, as described above.
\item
Five method servers, for \textbf{A}, \textbf{B}, \textbf{C}, \textbf{M}, and \textbf{N}.
\item
Five value servers, one per method server.
\item
Three trainers, one for each of \textbf{A}, \textbf{B}, and \textbf{C}.
\item
One executor and one store. 
\end{itemize}

\q{What about Q-trainer and Q value server?}

\q{How generic should the algorithm control module be?  Do we want something as generic as EMEWS?  }

%\begin{tabular}{| l }
%Component & Input & Output & Cost & Retraining 
%Generator & 
%Scorer       &
%Selector     &    
%Evaluator   & 








\section{Requirements for HPC system software}

We need mechanisms that can provide the following functionality on Summit and Theta/Aurora.

\begin{enumerate}
\item
Task creation and management within a single HPC session.
\begin{enumerate}
\item Creation of persistent and transient processes.We want to be able to allocate \textbf{N} $\ge$  \textbf{A} +  \textbf{B} +  \textbf{C} nodes and then:
\begin{enumerate}
\item
Start  \textbf{A} persistent manager processes, each on a single node
\item
Run many transient single-node tasks on another \textbf{B} nodes. (If using Parsl, that means starting B persistent worker processes on B nodes.)
\item
Run many transient multiple-node MPI computations on \textbf{C} nodes.
\end{enumerate}
\item
Where managers can monitor (e.g., detect failure) and manage (e.g., terminate, restart) their workers and MPI computations
\end{enumerate}
\item
Communication:
\begin{enumerate}
\item
Managers can pass information to the single-node tasks of (ii) above and the MPI computations of (iii) above, and get results back. E.g., conventional Parsl functional semantics.
\item
Managers can communicate in some convenient manner (e.g., message passing)
\item
Processes can also read and write a shared RAM / NVRAM store
\end{enumerate}
\end{enumerate}

For example, in the electrolyte design problem, we might have \textbf{A}=20, \textbf{B}=100, and \textbf{C}=880, for \textbf{N}=1000.


\section{Implementation approaches}

The process structures required to implement the task creation and management mechanisms can be implemented in a variety of ways:
\begin{itemize}
    \item
    Various systems provide system-dependent mechanisms for creating processes: e.g., \texttt{aprun} on Theta, \texttt{jsrun} on Summit. 
    \item
    The Process Management Interface -- Exascale (PMIx)~\cite{castain2018pmix}, \url{https://pmix.org}, defines an API for such capabilities.
    The PMIx-based Reference RunTime Environment (PRRTE) provides a reference implementation of this API. However, while PRRTE is available on Summit, it is not available on Theta.
    \item 
    MPICH has a process manager called Hydra, I'm not sure what that can do.
    \item
    Systems like Balsam, MPI\_Comm\_launch~\cite{wozniak2019mpi}, Swift, and Parsl~\cite{babuji19parsl} all provide various forms of task management.
    RADICAL-Pilot uses PRRTE on Summit.
\end{itemize}

It seems that the system-specific mechanisms provide us with what we need, 
albeit with quite low-level and non-portable interfaces. 
Other mechanisms provide greater portability and/or abstraction, 
but are not available on both Summit and Theta.
Thus we will likely want to use the Theta-specific mechanisms described in
the next section to move forward.

\subsection{Theta-specific mechanisms}

On Theta, the \texttt{qsub} command is used to create a \emph{job}, to which are allocated a specified number of compute nodes for a specified time.
Having created a job, the \texttt{aprun} command
can be used to launch
executables on one or more of the compute nodes allocated to the job.
The programmer can detect when an \texttt{aprun} command completes,
and launch new commands.
An \texttt{aprun} command can launch a single-node application or a multiple-node MPI 
application.
\q{As far as I can tell: a) two \texttt{aprun} commands cannot start executables on the 
same node, and b) a single \texttt{aprun} command can only start one executable. True?}
Once started, applications can communicate with each other via TCP/IP, for example
by using ZMQ. 
The \texttt{aprun} command can only be run on specialized Machine Oriented Mini-server (MOM) nodes, which are not to be used to run compute- or data-intensive applications. 

It would seem that we can use \texttt{aprun} to do everything that we need, as follows:
\begin{enumerate} 
\item 
We create a manager process on a MOM that is
responsible for launching all other processes.
\item\label{item:2}
The manager creates long-lived processes for the persistent tasks listed above:
the method servers, value servers, trainers, Parsl managers, etc. 
\item 
The manager creates long-lived processes to act as Parsl workers,
and passes the address of those workers to the method servers of step~\ref{item:2}
so that they can send function execution requests to them.
\q{Would we want the method server to send requests to a Parsl manager, which then
sends them to a Parsl worker? Or is that an unnecessary / unacceptable level of indirection?}
(Once configured in this way, a Parsl worker can retrieve and execute functions with $\sim$10ms overhead.)
\item 
The manager accepts subsequent requests to start MPI computations for simulation and trainer tasks. 
\q{How do the values produced by these computations get stored appropriately, without involving the manager? 
Maybe via the file system or an NVRAM virtual file system?}

\end{enumerate} 

\noindent
Notes:
\begin{itemize} 
\item
Yadu suggests that we can use a special version of the  Parsl manager, configured to run on a MOM, to start MPI tasks. Not clear whether that is desirable.
\item
 I don't yet know how to create a virtual file system or similar in RAM / NVRAM to hold intermediate results. (We likely should start by communicating via TCP or the file system.) [Zhao Zhang's CompresStore uses a ``trampoline'' to intercept I/O calls and pass them to his distributed cache process running on the same node, which retrieves the file contents, places it in the specified memory space, and then returns the memory address to the initial function.]
\end{itemize}

\vspace{1ex}

\noindent 
Note: \texttt{aprun} is the Application Level Placement Scheduler (ALPS) application launcher. That does not seem to be important information.

\subsection{MPI\_Comm\_launch}

\underline{Task creation and management}: 
The idea with \texttt{MPI\_Comm\_Launch} is that there should be some portable C/C++/Fortran
function that allows the execution of MPI subjobs within a greater MPI job, without the use of 
vendor or scheduler -specific job launcher tools, sockets, etc.  \texttt{MPI\_Comm\_Launch} 
would allow scientific codes to invoke other scientific codes to run subproblems or analysis.  It 
would also allow the development of portable application frameworks and generic workflow systems.

When \texttt{MPI\_Comm\_launch} is run on communicators created with \texttt{MPI\_Comm\_create\_group},
MPI subjob execution can be totally dynamic, and subjobs can be created with varying size or layout in 
response to application workflow needs.

My understanding is that with \texttt{MPI\_Comm\_launch}~\cite{wozniak2019mpi} and Parsl~\cite{babuji19parsl}, 
we can implement much of the task creation and management capabilities described above on a Linux cluster, but not currently on Summit or Theta.  

There has been some interest from Cray and the MPI Forum.  The Cray \texttt{MPIX\_Comm\_rankpool()} function provides some of the \texttt{MPI\_Comm\_launch} functionality and is available on Theta.  If we had a killer app for this, we could go back to these groups with more motivating examples to get more features.

Specifically, we can use \texttt{MPI\_Comm\_launch} first to start the persistent services and a set of Parsl workers, to which we can dispatch the fine-grained tasks,
and then to start sets of MPI computations, detect when individual computations terminate, and start new computations, as needed.
What we cannot do is to stop MPI computations once started. That is likely not important.

%\yadu{Parsl's general approach is to use the scheduler and system launcher (\texttt{aprun} on Theta, \texttt{jsrun} on Summit) to partition a job into a static set of chunks each with a Pilot(node-manager). Once partitioned, the managers retrieve and execute functions with $\sim$10ms overhead. Some of the slicing specification will be manual, but it is not hard. There are some rough edges here around affinitizing tasks to the right pilots and dealing with the launch mechanisms expected by MPI applications.}

\underline{Communication}: The various processes created in the ways just described can communicate via MPI mechanisms.
The question remains of how to create a Store.
\yadu{Do we have a guess for the volume and velocity? of the data in/out required of the store?}

\subsection{RADICAL-Pilot}

RADICAL-Pilot uses PMIx to create tasks on Summit~\cite{turilli2019characterizing}.

\yadu{Launching fine-grained tasks via the new PMIx interface which allows dynamically slicing a scheduler job after it is launched. I'd guess that launches are on the order of seconds not ms, fast enough to launch pilots, but not functions. Very cool functionality if the requirements of the train elements for multi-node MPI were to change}


\subsection{Other approaches?}

Balsam may be useful \url{https://balsam.readthedocs.io/en/latest/}.
Can it start persistent processes as well as manage queued jobs?

Dakota https://dakota.sandia.gov

Flux https://ieeexplore.ieee.org/document/8638377

\section{Other capabilities required}

\begin{enumerate}
    \item 
    \emph{Run NWChem/NWChemEx ensembles on Summit and Theta}.
    See \url{https://balsam.readthedocs.io/en/latest/tutorial/nwchem.html} for information on how to do this with Balsam on Theta.
    I have also asked Ray Bair as to whether he can help from the NWChemEx project.
\end{enumerate}

%\addcontentsline{toc}{chapter}{Literature cited}
\bibliographystyle{unsrt}
\bibliography{refs}

\addcontentsline{toc}{section}{Appendices}
\appendix
\appendixpage

\begin{comment}
\section{Service Descriptions}\label{sec:details}

We outline what we think are the internal state and operations supported by the method server, value server, trainer, and executor.

\vspace{3ex}

\noindent
\textbf{Method server}\\
State:
\begin{compactitem}
\item
Identity of method, $M$, to be applied
\item
Identity of associated value server, $S_V$
\item
Identify of executor, $E$
\end{compactitem}
Functions:
\begin{compactitem}
\item 
Score a design $D$ using method:
\begin{compactitem}
\item
Send task $M$(design) to $E$, and wait for returned $V$
\item
Send ($D$, $V$) to $S_V$
\end{compactitem}
\item
Update method (if ML method) 
\end{compactitem}

\vspace{3ex}

\noindent
\textbf{Value server}\\
State:
\begin{compactitem}
\item
Previously recorded values
\item
Clients to notify of new values (?)
\end{compactitem}
Functions:
\begin{compactitem}
\item
Record (key, value) pair(s) -- And notify registered clients (?)
\item
Retrieve specified or all values
\item
Archive old values on model update (?)
\end{compactitem}

\vspace{3ex}

\noindent
\textbf{Trainer}\\
State:
\begin{compactitem}
\item
Identify of value server, $S_V$
\item
Identify of method server, $S_M$
\item
Identity of executor, $E$
\end{compactitem}
Functions:
\begin{compactitem}
\item
Retrain ML model
\item
Retrieve values from $S_V$
\item
Send task to $E$ to retrain model 
\item
Update model in $S_M$
\end{compactitem}

\vspace{3ex}

\noindent
\textbf{Executor}\\
State:
\begin{compactitem}
\item
Currently active tasks
\item
Available resources
\end{compactitem}
Functions:
\begin{compactitem}
\item
Create a task, with specified executable, inputs, resources
\item
Monitor a task
\item
Manage a task
\item
Set policies for resource allocation (?)
\end{compactitem}
\end{comment}

\section{Pseudo-code}

\vspace{3ex}
\noindent
\textbf{Executor}

\noindent
This service is charged with creating the initial set of persistent services and with processing requests to create, and managing the execution of, transient MPI computations.

\begin{tabbing}
\=\=aaaa\=aaaa\=aaaa\=\kill
\>\> \texttt{requestQueue = []} \\
\>\> \# Create persistent processes and collect contact information \\
\>\> \texttt{for i in persistents:}\\
\>\>\> \texttt{info[i] = aprun(persistents[i])}\\
\>\> \texttt{distribute info to all persistent processes}\\
\>\> \texttt{freeNodes = numnodes() - size(persistents)}\\
\>\>\# Process events\\
\>\> \texttt{repeat:}\\
\>\>\> \texttt{if persistent process has failed:} \\
\>\>\>\> \texttt{panic()} \\
\>\>\> \texttt{if requested to start an MPI task $M$:}\\
\>\>\>\> \texttt{if size($M$) $<$= freeNodes:}\\
\>\>\>\>\> \texttt{start MPI task} \\
\>\>\>\>\> \texttt{freeNodes -= size($M$)}\\
\>\>\>\> \texttt{else:}\\
\>\>\>\>\> \texttt{queue($M$)}\\
\>\>\> \texttt{if an MPI task $T$ completes:}\\
\>\>\>\> \texttt{do something with the value returned}\\
\>\>\>\> \texttt{freeNodes += size($T$)}\\
\>\>\> \texttt{if queued task:} \\
\>\>\>\> \texttt{start queued task if there are enough free nodes}
\end{tabbing}


\vspace{3ex}
\noindent
\textbf{Method server}

\noindent
This service is configured with the address of the service to which it should send method execution requests (either the Executor, for MPI computations, or the Parsl manager, for lightweight functions), the address of its associated value server, and the details of its method. It responds to requests to execute methods 


\begin{tabbing}
\=\=aaaa\=aaaa\=aaaa\=\kill
\>\> \texttt{info = obtainInfoFromExecutor()}\\
\>\> \# \texttt{methodRunner} is Executor for simulation model or ParslManager for ML model\\
\>\> \texttt{methodRunner = extractRunnerInfo(info)}\\ 
\>\> \texttt{valueServer = extractValueServerInfo(info)}\\
\>\> \texttt{myMethod = initializeMethod()}\\
\>\>\# Process events\\
\>\> \texttt{repeat:}\\
\>\>\> \texttt{if method request, $R$:} \\
\>\>\>\> \texttt{send request myMethod($R$) to methodRunner}\\
\>\>\>\> \texttt{wait for value, $V$}\\
\>\>\>\> \texttt{send value ($R$, $V$) to valueServer} \\
\>\>\>\> \texttt{return value $V$ to requestor}\\
\>\>\> \texttt{if update request, $U$:} \\
\>\>\>\> \texttt{update myMethod with $U$}\\
\>\>\>\> \emph{maybe also tell valueServer to archive old data}\\
\>\>\>\> \texttt{acknowledge to requestor}
\end{tabbing}

\vspace{3ex}
\noindent
\textbf{Value server}

\noindent
This service processes requests from its associated method server to store values,
and (in the case of an ML method) from its associated trainer to retrieve values.
We may also want it to notify registered servers when a specified number of new values have been received, so that they can retrain.

\vspace{3ex}
\noindent
\textbf{Trainer}

\noindent
This service processes requests to retrain an ML model.
It will likely be configured with the address of the value server from which it should retrieve values.

\vspace{3ex}
\noindent
\textbf{Generator}

\noindent
This service is created by the Executor and supplied with the address of the 
Scorer and of any method server(s) that it needs to access, plus also information
about the design(s) that should be used as starting points for generation.
It repeatedly uses an RL algorithm to generate candidate designs,
which it passes on to the Scorer, either via a message or the file system.

\vspace{3ex}
\noindent
\textbf{Scorer}

\noindent
This service is created by the Executor and supplied with the address of the 
Selector and of any method server(s) that it needs to access.
It accepts designs from the Generator, scores them, and passes on the scored
designs to the Selector.

\vspace{3ex}
\noindent
\textbf{Selector}

\noindent
This service is created by the Executor and supplied with the address of the 
Evaluator and of any method server(s) that it needs to access.
It accepts designs from the Scorer, selects a subset of them,
and passes on that subset to the Evaluator.

\vspace{3ex}
\noindent
\textbf{Evaluator}

\noindent
This service is created by the Executor and supplied with the address of any method server(s) that it needs to access.
It accepts designs from the Selector and applies its configured methods(s) to each.

\end{document}
